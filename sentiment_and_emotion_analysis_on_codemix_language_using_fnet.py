# -*- coding: utf-8 -*-
"""Sentiment and Emotion analysis on Codemix language using Fnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10S2-PjqVPoGHOLEs-HwQSuxFe3ukqyqf
"""

import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
from tqdm import tqdm
import numpy as np
from sklearn.metrics import confusion_matrix, f1_score
import random
from transformers import XLMRobertaModel
import matplotlib.pyplot as plt
import seaborn as sns
import time

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define Fourier mixing sublayer
class FourierMixingSublayer(nn.Module):
    def __init__(self, dim):
        super(FourierMixingSublayer, self).__init__()
        self.dim = dim

    def forward(self, x):
        # Apply the Fourier transform along the last dimension
        x = torch.fft.fft(x, dim=-1)
        return torch.real(x)

# Modify Transformer encoder layer
class FNetTransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super(FNetTransformerEncoderLayer, self).__init__()
        self.config = config
        self.fft_sublayer = FourierMixingSublayer(config.hidden_size)
        self.feed_forward = nn.Linear(config.hidden_size, config.intermediate_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states):
        hidden_states = self.fft_sublayer(hidden_states)
        hidden_states = self.feed_forward(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states

class FNetXLMRobertaForSequenceClassification(nn.Module):
    def __init__(self, config, num_labels):
        super(FNetXLMRobertaForSequenceClassification, self).__init__()
        self.num_labels = num_labels
        self.roberta = XLMRobertaModel.from_pretrained('xlm-roberta-large')
        self.transformer_layer = FNetTransformerEncoderLayer(config)
        self.classifier = nn.Linear(config.intermediate_size, num_labels)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, attention_mask=None):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state

        last_hidden_state = self.transformer_layer(last_hidden_state)
        last_hidden_state = self.dropout(last_hidden_state)

        logits = self.classifier(last_hidden_state[:, 0, :])
        return logits

train_df = pd.read_csv(r'/kaggle/input/amit72/sentimix_train.csv')
test_df = pd.read_csv(r'/kaggle/input/amit72/sentimix_test.csv')
val_df = pd.read_csv(r'/kaggle/input/amit72/sentimix_val.csv')

# Define text preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = text.replace('[^\w\s]', '')
    text = text.strip()
    return text

# Preprocess text data
train_df['tweet'] = train_df['tweet'].apply(preprocess_text)
val_df['tweet'] = val_df['tweet'].apply(preprocess_text)
test_df['tweet'] = test_df['tweet'].apply(preprocess_text)

# Label encoding for sentiment and emotion labels
sentiment_label_encoder = LabelEncoder()
emotion_label_encoder = LabelEncoder()

train_df['sentiment_encoded'] = sentiment_label_encoder.fit_transform(train_df['sentiment'])
val_df['sentiment_encoded'] = sentiment_label_encoder.transform(val_df['sentiment'])
test_df['sentiment_encoded'] = sentiment_label_encoder.transform(test_df['sentiment'])
train_df['emotion_encoded'] = emotion_label_encoder.fit_transform(train_df['emotion'])
val_df['emotion_encoded'] = emotion_label_encoder.transform(val_df['emotion'])
test_df['emotion_encoded'] = emotion_label_encoder.transform(test_df['emotion'])

# Load XLM-Roberta tokenizer
tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')

config = AutoConfig.from_pretrained('xlm-roberta-large')
config.num_labels = len(sentiment_label_encoder.classes_) + len(emotion_label_encoder.classes_)

# Load model
model = FNetXLMRobertaForSequenceClassification(config, num_labels=config.num_labels)
model.to(device)

# Custom dataset class
class SentimentAndEmotionDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        tweet = str(self.data.iloc[idx]['tweet'])
        sentiment_label = int(self.data.iloc[idx]['sentiment_encoded'])
        emotion_label = int(self.data.iloc[idx]['emotion_encoded'])

        encoding = self.tokenizer(tweet, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)

        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()
        sentiment_label = torch.tensor(sentiment_label, dtype=torch.long)
        emotion_label = torch.tensor(emotion_label, dtype=torch.long)

        return {
            'input_ids': input_ids.to(device),
            'attention_mask': attention_mask.to(device),
            'sentiment_label': sentiment_label.to(device),
            'emotion_label': emotion_label.to(device)
        }

# Create training and validation datasets
train_dataset = SentimentAndEmotionDataset(train_df, tokenizer)
val_dataset = SentimentAndEmotionDataset(val_df, tokenizer)
test_dataset = SentimentAndEmotionDataset(test_df, tokenizer)

# Create training and validation data loaders
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=2e-5)

def plot_confusion_matrices(epoch, sentiment_true, sentiment_pred, emotion_true, emotion_pred, sentiment_labels, emotion_labels):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Plot confusion matrix for sentiment
    sentiment_cm = confusion_matrix(sentiment_true, sentiment_pred)
    sns.heatmap(sentiment_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], xticklabels=sentiment_labels, yticklabels=sentiment_labels)
    axes[0].set_title(f'Sentiment Confusion Matrix - Epoch {epoch + 1}')
    axes[0].set_xlabel('Predicted')
    axes[0].set_ylabel('True')

    # Plot confusion matrix for emotion
    emotion_cm = confusion_matrix(emotion_true, emotion_pred)
    sns.heatmap(emotion_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1], xticklabels=emotion_labels, yticklabels=emotion_labels)
    axes[1].set_title(f'Emotion Confusion Matrix - Epoch {epoch + 1}')
    axes[1].set_xlabel('Predicted')
    axes[1].set_ylabel('True')

    plt.tight_layout()
    plt.show()

loss_fn = CrossEntropyLoss()
best_val_accuracy = 0.0
patience = 25
no_improvement = 0
best_epoch = 0
inference_times = []
original_inference = []

# Training loop
for epoch in range(10):
    start_time = time.time()
    model.train()
    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}'):
        optimizer.zero_grad()
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        sentiment_labels = batch['sentiment_label']
        emotion_labels = batch['emotion_label']

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs

        sentiment_logits = logits[:, :len(sentiment_label_encoder.classes_)]
        emotion_logits = logits[:, len(sentiment_label_encoder.classes_):]
        sentiment_logits = torch.cat((sentiment_logits, emotion_logits), dim=1)
        sentiment_loss = loss_fn(sentiment_logits, sentiment_labels)
        emotion_loss = loss_fn(emotion_logits, emotion_labels)

        total_loss = 0.3 * sentiment_loss + 0.3 * emotion_loss
        total_loss.backward()
        optimizer.step()
    end_time = time.time()
    epoch_time = end_time - start_time
    inference_times.append(epoch_time)
    print(f'Epoch {epoch + 1} Inference Time: {epoch_time:.2f} seconds')

    # Evaluation on validation dataset after each epoch
    model.eval()
    start_time = time.time()
    correct_sentiment_predictions = 0
    total_samples = 0
    for batch in val_dataloader:
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        sentiment_labels = batch['sentiment_label']
        emotion_labels = batch['emotion_label']

        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs

            sentiment_logits = logits[:, :len(sentiment_label_encoder.classes_)]
            emotion_logits = logits[:, len(sentiment_label_encoder.classes_):]

            sentiment_predicted_labels = torch.argmax(sentiment_logits, dim=1)

            correct_sentiment_predictions += torch.sum(sentiment_predicted_labels == sentiment_labels).item()
            total_samples += len(sentiment_labels)
    end_time = time.time()
    epoch_time = end_time - start_time
    original_inference.append(epoch_time)
    print(f'Epoch {epoch + 1} original Inference Time: {epoch_time:.2f} seconds')
    val_accuracy = correct_sentiment_predictions / total_samples
    print(f'Epoch {epoch + 1} - Validation Accuracy:', val_accuracy)

    sentiment_predictions = []
    emotion_predictions = []
    sentiment_true_labels = []
    emotion_true_labels = []

    for batch in test_dataloader:
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        sentiment_labels = batch['sentiment_label']
        emotion_labels = batch['emotion_label']

        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs

            sentiment_logits = logits[:, :len(sentiment_label_encoder.classes_)]
            emotion_logits = logits[:, len(sentiment_label_encoder.classes_):]

            sentiment_predicted_labels = torch.argmax(sentiment_logits, dim=1)
            emotion_predicted_labels = torch.argmax(emotion_logits, dim=1)

        sentiment_predictions.extend(sentiment_predicted_labels.cpu().numpy())
        emotion_predictions.extend(emotion_predicted_labels.cpu().numpy())
        sentiment_true_labels.extend(sentiment_labels.cpu().numpy())
        emotion_true_labels.extend(emotion_labels.cpu().numpy())

    # Convert to NumPy arrays before calculating accuracy
    sentiment_predictions = np.array(sentiment_predictions)
    emotion_predictions = np.array(emotion_predictions)
    sentiment_true_labels = np.array(sentiment_true_labels)
    emotion_true_labels = np.array(emotion_true_labels)

    # Calculate evaluation metrics for sentiment
    sentiment_accuracy = (sentiment_predictions == sentiment_true_labels).mean()
    sentiment_f1 = f1_score(sentiment_true_labels, sentiment_predictions, average='weighted')
    print('Test Set - Sentiment Accuracy:', sentiment_accuracy)
    print('Test Set - Sentiment F1 Score:', sentiment_f1)
    emotion_accuracy = (emotion_predictions == emotion_true_labels).mean()
    print('Test Set -  emotion_accuracy:', emotion_accuracy)
    emotion_f1 = f1_score(emotion_true_labels, emotion_predictions, average='weighted')
    print('Test Set - emotion F1 Score:', emotion_f1)

    # Confusion Matrix for Sentiment
    sentiment_conf_matrix = confusion_matrix(sentiment_true_labels, sentiment_predictions)
    print('Test Set - Sentiment Confusion Matrix:\n', sentiment_conf_matrix)

    # Confusion Matrix for Emotion
    emotion_conf_matrix = confusion_matrix(emotion_true_labels, emotion_predictions)
    print('Test Set - Emotion Confusion Matrix:\n', emotion_conf_matrix)

    plot_confusion_matrices(epoch, sentiment_true_labels, sentiment_predictions, emotion_true_labels, emotion_predictions, sentiment_label_encoder.classes_, emotion_label_encoder.classes_)

# Evaluation on test dataset after completing training
model.eval()
sentiment_predictions = []
emotion_predictions = []
sentiment_true_labels = []
emotion_true_labels = []

for batch in test_dataloader:
    input_ids = batch['input_ids']
    attention_mask = batch['attention_mask']
    sentiment_labels = batch['sentiment_label']
    emotion_labels = batch['emotion_label']

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs

        sentiment_logits = logits[:, :len(sentiment_label_encoder.classes_)]
        emotion_logits = logits[:, len(sentiment_label_encoder.classes_):]

        sentiment_predicted_labels = torch.argmax(sentiment_logits, dim=1)
        emotion_predicted_labels = torch.argmax(emotion_logits, dim=1)

    sentiment_predictions.extend(sentiment_predicted_labels.cpu().numpy())
    emotion_predictions.extend(emotion_predicted_labels.cpu().numpy())
    sentiment_true_labels.extend(sentiment_labels.cpu().numpy())
    emotion_true_labels.extend(emotion_labels.cpu().numpy())

# Convert to NumPy arrays before calculating accuracy
sentiment_predictions = np.array(sentiment_predictions)
emotion_predictions = np.array(emotion_predictions)
sentiment_true_labels = np.array(sentiment_true_labels)
emotion_true_labels = np.array(emotion_true_labels)

# Calculate evaluation metrics for sentiment
sentiment_accuracy = (sentiment_predictions == sentiment_true_labels).mean()
sentiment_f1 = f1_score(sentiment_true_labels, sentiment_predictions, average='weighted')
print('Test Set - Sentiment Accuracy:', sentiment_accuracy)
print('Test Set - Sentiment F1 Score:', sentiment_f1)

# Calculate evaluation metrics for emotion
emotion_accuracy = (emotion_predictions == emotion_true_labels).mean()
emotion_f1 = f1_score(emotion_true_labels, emotion_predictions, average='weighted')
print('Test Set - Emotion Accuracy:', emotion_accuracy)
print('Test Set - Emotion F1 Score:', emotion_f1)

# Confusion Matrix for Sentiment
sentiment_conf_matrix = confusion_matrix(sentiment_true_labels, sentiment_predictions)
print('Test Set - Sentiment Confusion Matrix:\n', sentiment_conf_matrix)

# Confusion Matrix for Emotion
emotion_conf_matrix = confusion_matrix(emotion_true_labels, emotion_predictions)
print('Test Set - Emotion Confusion Matrix:\n', emotion_conf_matrix)

plt.plot(range(1, 11), inference_times[:10])  # Plotting for 10 epochs
plt.xlabel('Epoch')
plt.ylabel('Inference Time (seconds)')
plt.title('Inference Time per Epoch (First 10 Epochs)')
plt.show()

